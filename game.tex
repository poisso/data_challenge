% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={R Notebook},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{R Notebook}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"sample\_submission.csv"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(example)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Id Predicted
## 1  0         1
## 2  1         0
## 3  2         0
## 4  3         0
## 5  4         0
## 6  5         1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"test.csv"}\NormalTok{)}
\NormalTok{train }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"train.csv"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    X0  X1  X2  X3  X4  X5  X6  X7  X8  X9 X10 X11 X12 X13 X14 X15 X16 X17 X18
## 1 481 514 501 483 486 486 427 477 480 487 575 497 470 499 502 468 494 516 460
## 2 472 442 495 490 557 481 555 476 487 485 514 491 447 449 454 485 504 510 527
## 3 472 465 618 494 456 483 444 477 483 480 528 483 494 439 489 466 482 467 495
## 4 491 462 513 491 450 477 520 474 459 484 446 469 542 497 486 501 493 553 464
## 5 486 487 527 477 502 477 503 476 467 468 514 475 492 445 494 476 499 463 483
## 6 487 525 432 480 541 467 483 477 463 485 540 515 469 511 477 503 462 505 478
##   X19 X20 X21 X22 X23 X24 X25 X26 X27 X28 X29 X30 X31 X32 X33 X34 X35 X36 X37
## 1 485 461 466 470 473 511 537 487 471 467 482 486 523 450 427 476 510 478 507
## 2 476 498 476 479 488 581 524 524 470 501 522 479 521 558 488 484 498 478 444
## 3 467 492 484 483 492 477 485 503 482 438 458 485 467 447 574 477 534 479 419
## 4 476 502 514 479 485 441 503 481 463 500 501 484 415 508 573 534 476 464 489
## 5 478 496 504 472 467 521 521 461 480 493 506 483 540 474 559 507 491 492 516
## 6 495 482 482 473 493 559 517 481 491 469 519 474 519 501 497 504 515 482 450
##   X38 X39 X40 X41 X42 X43 X44 X45 X46 X47 X48 X49 X50 X51 X52 X53 X54 X55 X56
## 1 478 474 474 514 469 499 510 469 506 440 482 486 641 554 459 477 537 516 493
## 2 484 476 474 475 482 498 472 467 478 458 581 440 527 417 471 474 497 494 566
## 3 477 478 475 490 480 455 494 463 491 534 485 480 564 487 460 482 485 485 500
## 4 473 476 475 515 470 469 501 481 509 460 523 499 489 523 443 479 471 535 525
## 5 480 476 481 528 490 497 517 478 418 506 556 474 627 499 498 478 476 469 493
## 6 472 476 474 455 506 462 534 437 568 500 447 479 561 511 487 479 513 459 462
##   X57 X58 X59 X60 X61 X62 X63 X64 X65 X66 X67 X68 X69 X70 X71 X72 X73 X74 X75
## 1 486 484 502 482 476 484 479 524 503 493 490 480 476 531 493 496 463 477 451
## 2 449 564 510 484 485 489 485 483 531 526 450 472 476 532 493 500 407 476 500
## 3 460 512 456 473 484 519 480 521 516 496 486 475 468 492 485 506 503 472 542
## 4 529 515 496 502 479 465 484 377 531 505 514 490 468 520 493 464 503 470 478
## 5 456 475 481 494 495 521 482 579 575 448 467 478 481 521 496 509 432 488 581
## 6 486 494 487 486 478 495 494 398 535 530 502 475 472 462 491 491 470 470 452
##   X76 X77 X78 X79 X80 X81 X82 X83 X84 X85 X86 X87 X88 X89 X90 X91 X92 X93 X94
## 1 460 432 518 470 493 506 506 474 511 526 478 501 474 529 476 512 493 480 496
## 2 477 508 503 520 458 538 512 482 502 483 473 486 475 521 477 475 506 535 481
## 3 475 493 439 494 459 453 494 487 510 522 479 487 473 519 475 505 497 499 482
## 4 479 445 497 475 464 425 508 481 472 496 476 472 480 505 476 442 471 488 474
## 5 480 465 500 524 571 520 539 467 449 469 476 510 489 500 476 473 495 529 490
## 6 457 469 573 447 467 553 443 476 517 425 478 488 482 533 476 479 487 469 475
##   X95 X96 X97 X98 X99 X100 X101 X102 X103 X104 X105 X106 X107 X108 X109 X110
## 1 534 483 450 494 489  479  471  480  494  483  457  494  463  523  457  443
## 2 510 479 476 501 499  486  491  482  485  477  706  475  537  494  477  473
## 3 520 480 464 515 513  484  489  485  441  442  437  505  458  490  460  501
## 4 557 485 458 516 429  474  479  487  515  498  815  488  508  516  466  431
## 5 445 478 467 478 504  478  480  483  492  562  676  484  497  428  497  500
## 6 487 478 470 464 458  478  478  487  455  479  440  508  494  503  481  448
##   X111 X112 X113 X114 X115 X116 X117 X118 X119 X120 X121 X122 X123 X124 X125
## 1  481  486  521  482  464  522  472  494  488  476  491  494  477  465  512
## 2  462  469  489  416  507  494  470  454  537  477  435  477  439  453  555
## 3  468  491  475  510  476  510  516  463  475  475  464  490  470  516  461
## 4  492  474  497  481  456  513  488  517  484  475  513  476  355  490  493
## 5  500  480  522  542  467  484  478  487  478  479  488  484  446  492  472
## 6  471  482  451  496  488  484  452  498  509  479  462  488  472  486  502
##   X126 X127 X128 X129 X130 X131 X132 X133 X134 X135 X136 X137 X138 X139 X140
## 1  470  472  475  472  488  530  456  466  459  468  498  502  479  508  484
## 2  514  527  496  472  467  478  503  481  497  536  425  487  473  519  492
## 3  478  530  473  510  497  507  448  474  518  530  507  477  486  491  502
## 4  491  505  510  452  493  484  481  473  491  628  450  500  476  479  498
## 5  469  487  488  483  470  468  518  475  480  464  433  490  491  518  470
## 6  457  462  473  512  560  499  486  485  461  575  543  492  476  499  489
##   X141 X142 X143 X144 X145 X146 X147 X148 X149 X150 X151 X152 X153 X154 X155
## 1  440  485  486  482  483  481  531  476  430  518  487  485  441  478  483
## 2  506  466  487  490  480  485  547  496  455  492  487  473  547  478  496
## 3  454  483  469  484  521  473  434  477  501  468  482  471  590  477  444
## 4  525  482  477  461  494  482  474  482  456  449  481  474  667  475  486
## 5  521  464  478  504  479  476  515  486  523  450  480  477  675  476  464
## 6  482  463  484  493  471  479  420  477  343  539  472  474  489  479  495
##   X156 X157 X158 X159 X160 X161 X162 X163 X164 X165 X166 X167 X168 X169 X170
## 1  469  495  473  534  510  492  484  472  454  477  476  483  477  588  529
## 2  494  441  537  495  489  492  507  485  489  490  479  478  478  415  480
## 3  521  482  505  507  492  489  479  476  435  465  474  481  477  422  584
## 4  486  433  526  501  526  475  496  475  519  464  476  475  478  558  445
## 5  473  460  466  505  471  462  492  488  517  487  480  482  476  559  464
## 6  467  467  440  475  457  466  482  479  448  522  476  481  476  451  448
##   X171 X172 X173 X174 X175 X176 X177 X178 X179 X180 X181 X182 X183 X184 X185
## 1  441  496  479  442  438  499  471  513  477  508  488  466  497  478  535
## 2  547  510  476  462  417  519  479  497  509  592  584  483  473  477  514
## 3  562  528  476  476  553  448  484  496  461  526  499  471  474  497  514
## 4  492  502  476  506  517  490  484  480  486  502  563  466  461  494  500
## 5  454  446  479  486  476  460  476  530  513  502  448  528  456  501  486
## 6  518  482  476  457  439  479  483  541  489  512  520  461  462  468  487
##   X186 X187 X188 X189 X190 X191 X192 X193 X194 X195 X196 X197 X198 X199 X200
## 1  541  486  575  485  454  477  495  423  544  472  478  484  478  516  461
## 2  505  488  555  485  509  497  479  569  469  495  485  476  476  465  477
## 3  524  501  559  478  462  427  468  496  520  473  483  497  477  513  474
## 4  499  472  456  478  469  531  499  539  530  458  480  479  477  525  502
## 5  505  510  516  482  507  509  514  514  570  472  480  483  476  434  461
## 6  507  486  496  480  528  531  496  514  602  480  470  491  477  481  478
##   X201 X202 X203 X204 X205 X206 X207 X208 X209 X210 X211 X212 X213 X214 X215
## 1  470  486  528  511  477  487  483  465  498  495  460  491  514  467  548
## 2  494  485  416  403  474  537  480  458  472  500  529  471  509  476  538
## 3  483  458  550  496  476  452  479  481  488  474  498  473  527  474  531
## 4  484  481  484  460  474  495  482  499  503  496  442  503  428  475  418
## 5  477  499  523  484  475  554  476  464  497  492  424  508  463  485  465
## 6  444  502  495  490  476  520  479  497  512  490  464  482  476  472  434
##   X216 X217 X218 X219 X220 X221 X222 X223 X224 X225 X226 X227 X228 X229 X230
## 1  501  505  459  491  444  511  497  496  448  466  511  477  477  497  474
## 2  506  480  486  478  485  498  523  472  514  479  457  474  476  477  488
## 3  502  477  490  506  459  540  486  478  527  515  484  476  476  488  491
## 4  533  470  466  478  525  483  466  461  518  528  512  477  477  505  450
## 5  491  492  509  460  489  500  489  431  493  486  518  477  477  499  467
## 6  465  476  498  476  494  434  442  539  497  556  485  479  476  492  470
##   X231 X232 X233 X234 X235 X236 X237 X238 X239 X240 X241 X242 X243 X244 X245
## 1  459  476  490  493  504  496  477  462  474  462  407  514  482  574  487
## 2  505  479  499  484  471  496  474  477  483  493  566  473  495  508  506
## 3  468  484  487  490  497  493  473  540  481  514  458  476  470  537  532
## 4  471  462  498  483  525  471  482  470  486  527  588  445  499  500  529
## 5  502  463  475  499  469  467  477  499  476  511  509  492  496  480  512
## 6  498  483  481  483  518  506  476  449  479  487  600  555  467  536  422
##   X246 X247 X248 X249 X250 X251 X252 X253 X254 X255 X256 X257 X258 X259 X260
## 1  443  475  479  501  504  471  479  512  482  463  449  481  480  503  461
## 2  445  481  489  541  459  508  479  491  479  489  570  507  490  484  494
## 3  527  480  476  523  511  491  475  454  469  423  489  507  472  478  514
## 4  490  476  487  547  453  529  476  467  475  520  465  522  470  495  506
## 5  450  480  469  454  465  465  477  506  473  495  523  498  509  488  498
## 6  487  478  492  509  481  486  479  526  480  468  450  463  483  486  458
##   X261 X262 X263 X264 X265 X266 X267 X268 X269 X270 X271 X272 X273 X274 X275
## 1  481  508  493  483  507  499  493  538  477  468  436  409  532  456  497
## 2  478  467  517  460  498  534  475  500  484  491  492  504  481  455  485
## 3  479  488  448  479  452  537  465  500  467  501  561  432  485  481  487
## 4  486  476  494  465  500  518  486  448  473  479  469  464  531  472  496
## 5  482  509  489  486  473  559  473  463  523  479  544  521  503  487  472
## 6  479  501  464  496  494  505  503  531  494  482  423  458  497  459  484
##   X276 X277 X278 X279 X280 X281 X282 X283 X284 X285 X286 X287 X288 X289 X290
## 1  476  500  482  494  476  448  460  478  482  496  494  496  471  508  479
## 2  477  456  479  479  476  508  486  476  516  488  550  439  473  503  514
## 3  476  536  500  494  475  518  474  479  492  499  481  490  484  519  505
## 4  476  500  487  516  475  573  488  478  494  476  446  474  486  473  499
## 5  475  499  482  455  479  570  478  476  514  494  500  466  481  476  511
## 6  476  447  514  495  477  475  498  475  580  456  487  500  479  439  471
##   X291 X292 X293 X294 X295 X296 X297 X298 X299 X300 X301 X302 X303 X304 X305
## 1  525  474  476  483  464  449  504  521  478  413  458  476  520  536  462
## 2  498  510  472  439  500  480  530  475  491  449  479  473  476  572  478
## 3  508  480  475  408  485  476  507  495  480  455  486  476  515  467  484
## 4  435  514  475  476  461  480  475  486  521  451  515  475  519  502  498
## 5  524  478  481  483  510  461  479  457  497  463  463  481  515  476  483
## 6  499  488  467  518  513  480  490  452  498  522  510  480  487  496  467
##   X306 X307 X308 X309 X310 X311 X312 X313 X314 X315 X316 X317 X318 X319 X320
## 1  477  471  455  525  525  469  479  505  481  526  462  471  438  517  485
## 2  486  491  530  450  511  498  505  468  467  502  478  471  540  517  494
## 3  457  482  483  489  534  479  514  524  482  503  464  472  358  454  476
## 4  497  487  551  497  489  484  500  496  472  477  462  479  544  490  479
## 5  504  479  568  488  437  479  531  476  478  495  468  481  529  474  500
## 6  476  487  550  426  524  490  464  505  480  514  482  483  450  528  490
##   X321 X322 X323 X324 X325 X326 X327 X328 X329 X330 X331 X332 X333 X334 X335
## 1  508  454  476  492  480  481  487  500  493  506  462  476  449  533  493
## 2  504  469  465  462  478  495  479  483  515  463  476  476  451  486  505
## 3  449  503  491  495  474  426  444  456  491  509  486  476  510  449  483
## 4  512  504  512  477  476  488  487  489  586  480  486  477  503  507  493
## 5  522  486  465  472  473  503  512  511  549  491  472  475  527  500  504
## 6  468  466  475  469  477  434  444  474  560  504  472  478  437  457  496
##   X336 X337 X338 X339 X340 X341 X342 X343 X344 X345 X346 X347 X348 X349 X350
## 1  521  524  598  508  521  499  522  562  495  491  485  492  488  468  474
## 2  463  508  358  451  506  480  502  474  564  486  471  539  480  456  478
## 3  509  494  566  501  566  480  522  511  550  474  487  473  472  503  474
## 4  338  478  221  499  454  464  451  465  570  486  458  485  510  493  476
## 5  575  545  343  576  519  519  494  490  498  490  490  501  474  491  474
## 6  357  518  451  473  555  467  542  492  556  487  486  477  507  493  474
##   X351 X352 X353 X354 X355 X356 X357 X358 X359 X360 X361 X362 X363 X364 X365
## 1  524  496  487  502  475  475  472  468  458  498  502  524  439  480  498
## 2  523  461  465  481  492  417  475  515  468  392  499  502  518  482  563
## 3  573  490  477  484  498  474  475  487  456  507  501  486  453  489  431
## 4  532  454  482  492  478  462  476  579  506  456  473  499  479  481  515
## 5  486  485  479  505  488  480  476  551  493  493  489  434  415  474  462
## 6  517  524  457  462  492  527  478  512  509  475  527  456  470  479  512
##   X366 X367 X368 X369 X370 X371 X372 X373 X374 X375 X376 X377 X378 X379 X380
## 1  492  503  498  509  478  465  487  484  461  490  488  482  491  489  481
## 2  458  478  482  495  412  454  473  468  525  520  483  484  602  455  482
## 3  520  502  489  472  503  500  503  490  453  450  482  517  498  493  473
## 4  520  482  470  454  460  463  490  490  518  511  473  494  554  517  485
## 5  483  468  486  555  526  574  500  471  488  501  486  501  577  490  478
## 6  487  480  464  497  431  462  527  437  542  480  487  491  450  489  480
##   X381 X382 X383 X384 X385 X386 X387 X388 X389 X390 X391 X392 X393 X394 X395
## 1  486  411  497  492  467  497  477  510  497  475  531  475  513  477  458
## 2  509  428  475  476  457  516  476  459  538  479  540  476  496  483  503
## 3  469  544  485  487  478  500  477  485  468  470  496  479  460  475  504
## 4  477  540  482  511  473  514  477  488  498  475  488  477  530  483  512
## 5  464  515  467  468  478  507  478  463  476  479  462  480  490  483  496
## 6  496  509  475  465  461  498  475  493  487  480  455  477  522  474  518
##   X396 X397 X398 X399 X400 X401 X402 X403 X404 X405 X406 X407 X408 X409 X410
## 1  483  484  531  481  479  514  473  496  477  477  471  475  446  477  487
## 2  475  477  519  495  477  498  476  511  476  486  500  479  464  478  495
## 3  490  496  490  484  468  443  477  447  476  478  556  480  500  477  482
## 4  467  362  461  482  473  501  473  480  476  480  457  473  467  478  466
## 5  477  544  473  486  477  507  476  503  476  469  485  485  472  484  490
## 6  483  450  496  480  476  494  477  508  476  484  471  473  540  469  462
##   X411 X412 X413 X414 X415 X416 X417 X418 X419 X420 X421 X422 X423 X424 X425
## 1  454  451  465  457  565  470  451  511  554  490  478  459  476  476  485
## 2  496  467  463  539  489  473  436  636  508  480  493  492  475  487  469
## 3  473  481  501  479  437  491  470  537  532  480  472  466  476  471  488
## 4  438  435  456  537  556  506  559  495  474  469  472  484  477  475  487
## 5  463  500  497  466  614  490  489  482  515  487  467  470  476  484  471
## 6  492  557  478  518  542  464  454  566  456  481  473  472  476  477  500
##   X426 X427 X428 X429 X430 X431 X432 X433 X434 X435 X436 X437 X438 X439 X440
## 1  468  499  464  479  513  532  455  460  482  451  467  456  493  495  523
## 2  499  451  512  480  514  497  436  539  487  463  479  536  514  484  480
## 3  499  482  453  478  477  442  482  569  489  518  479  565  512  485  482
## 4  489  472  486  468  506  540  503  596  511  451  475  464  451  489  475
## 5  528  489  489  477  515  567  494  625  488  494  484  501  468  499  536
## 6  480  515  456  481  472  479  464  490  473  502  485  460  456  482  478
##   X441 X442 X443 X444 X445 X446 X447 X448 X449 X450 X451 X452 X453 X454 X455
## 1  486  700  497  483  478  487  498  460  472  506  469  472  616  475  423
## 2  479  285  522  494  477  488  469  461  459  481  490  488  417  482  556
## 3  466  710  481  499  476  507  495  500  487  481  455  498  699  473  431
## 4  487  319  538  489  480  475  474  492  462  497  489  463  580  472  402
## 5  465  308  514  477  477  463  486  410  476  488  488  478  562  483  533
## 6  491  458  517  500  478  468  500  461  470  494  470  474  338  474  540
##   X456 X457 X458 X459 X460 X461 X462 X463 X464 X465 X466 X467 X468 X469 X470
## 1  481  485  485  497  530  433  495  515  496  471  484  521  473  447  522
## 2  475  494  532  502  507  474  470  493  499  469  485  471  484  434  556
## 3  486  433  548  471  509  496  577  521  470  467  479  500  469  497  501
## 4  482  529  477  521  517  496  460  448  461  499  485  500  448  507  573
## 5  475  478  409  516  466  465  505  531  475  484  468  480  456  462  488
## 6  481  476  381  550  466  519  448  601  474  515  489  536  459  485  478
##   X471 X472 X473 X474 X475 X476 X477 X478 X479 X480 X481 X482 X483 X484 X485
## 1  482  542  476  474  387  467  467  494  480  494  463  473  480  437  493
## 2  477  379  477  472  570  474  517  462  500  469  442  471  518  507  491
## 3  479  576  473  482  430  492  479  522  486  484  535  470  514  504  485
## 4  479  396  479  481  621  465  491  456  493  479  456  476  486  518  498
## 5  474  408  479  474  496  452  480  459  474  500  478  475  520  542  513
## 6  486  463  478  483  633  495  482  412  500  470  426  489  497  528  486
##   X486 X487 X488 X489 X490 X491 X492 X493 X494 X495 X496 X497 X498 X499 target
## 1  491  453  481  471  488  483  490  629  475  508  478  490  500  527      0
## 2  485  459  447  477  464  478  460  411  502  547  486  479  517  509      1
## 3  479  446  472  496  448  476  497  726  473  565  468  486  460  477      1
## 4  491  479  440  476  478  480  486  625  458  495  482  476  485  458      1
## 5  493  563  525  475  503  482  468  583  505  509  482  479  499  515      0
## 6  478  497  460  463  503  477  523  342  443  580  477  509  530  513      1
\end{verbatim}

To get a better understanding of our data lets try and observe the
standard deviation of a random sample of features.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dev }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{200}\NormalTok{)}
\NormalTok{c }\OtherTok{\textless{}{-}} \DecValTok{1}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{500}\NormalTok{, }\DecValTok{200}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)) \{}
\NormalTok{  dev[c] }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(train[,}\FunctionTok{c}\NormalTok{(i)])}
\NormalTok{  c }\OtherTok{=}\NormalTok{ c }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{\}}
\FunctionTok{plot}\NormalTok{(dev)}
\end{Highlighting}
\end{Shaded}

\includegraphics{game_files/figure-latex/unnamed-chunk-3-1.pdf} We
conclude from this analysis that the standard deviation for most
features is contained between 0 and 40 but a few columns go higher than
this threshold we could maybe take advantage of this later.

Lets try to do the same analysis after getting rid of some extreme
values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ train}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{500}\NormalTok{)\{}
\NormalTok{  moy }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(x[,i])}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{1500}\NormalTok{) \{}
    \ControlFlowTok{if}\NormalTok{ (x[j, i] }\SpecialCharTok{\textgreater{}} \DecValTok{700} \SpecialCharTok{||}\NormalTok{ x[j, i] }\SpecialCharTok{\textless{}} \DecValTok{300}\NormalTok{)\{}
\NormalTok{      x[j, i] }\OtherTok{\textless{}{-}}\NormalTok{ moy}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}
\NormalTok{dev }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{200}\NormalTok{)}
\NormalTok{c }\OtherTok{\textless{}{-}} \DecValTok{1}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{500}\NormalTok{, }\DecValTok{200}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)) \{}
\NormalTok{  dev[c] }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(x[,}\FunctionTok{c}\NormalTok{(i)])}
\NormalTok{  c }\OtherTok{=}\NormalTok{ c }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{\}}
\FunctionTok{plot}\NormalTok{(dev)}
\end{Highlighting}
\end{Shaded}

\includegraphics{game_files/figure-latex/unnamed-chunk-4-1.pdf} There is
no significant improvement in the analysis.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'MASS' was built under R version 4.0.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'class' was built under R version 4.0.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nnet)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'nnet' was built under R version 4.0.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(caret)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'caret' was built under R version 4.0.5
\end{verbatim}

\begin{verbatim}
## Loading required package: lattice
\end{verbatim}

\begin{verbatim}
## Loading required package: ggplot2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(klaR)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'klaR' was built under R version 4.0.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(train, }\AttributeTok{center=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{scale=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{variance }\OtherTok{=}\NormalTok{ (pca}\SpecialCharTok{$}\NormalTok{sdev)}\SpecialCharTok{**}\DecValTok{2} 
\NormalTok{inertia }\OtherTok{=}\NormalTok{ variance}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(variance)  }
\NormalTok{result }\OtherTok{=} \FunctionTok{cumsum}\NormalTok{(inertia)[}\DecValTok{300}\NormalTok{]}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Inertia captured by the first 300 principal components: "}\NormalTok{, result)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Inertia captured by the first 300 principal components:  0.8331288
\end{verbatim}

We can see that the data cannot be explained by a small number of
variables, thus it is very difficult to actually observe the data and
understand it in a simple way.

For example if we try to observe the data projected on the 2 first
components of the pca we get this plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(devtools)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'devtools' was built under R version 4.0.5
\end{verbatim}

\begin{verbatim}
## Loading required package: usethis
\end{verbatim}

\begin{verbatim}
## Warning: package 'usethis' was built under R version 4.0.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install\_github}\NormalTok{(}\StringTok{"vqv/ggbiplot"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## WARNING: Rtools is required to build R packages, but is not currently installed.
## 
## Please download and install Rtools 4.0 from https://cran.r-project.org/bin/windows/Rtools/.
\end{verbatim}

\begin{verbatim}
## Skipping install of 'ggbiplot' from a github remote, the SHA1 (7325e880) has not changed since last install.
##   Use `force = TRUE` to force installation
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggbiplot)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: plyr
\end{verbatim}

\begin{verbatim}
## Loading required package: scales
\end{verbatim}

\begin{verbatim}
## Loading required package: grid
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggbiplot}\NormalTok{(pca)}
\end{Highlighting}
\end{Shaded}

\includegraphics{game_files/figure-latex/unnamed-chunk-9-1.pdf}

These 2 components explain only a few percents of the variability of the
data and the plot we get is centered and apparently distributed in a
pretty uniform way.

We will test different prediction methods to try and predict the data
properly. To get a hint about the accuracy of our method we are going to
seperate our training data into 2 groups, 20\% of the data will be use
for testing and getting an accuracy calculation of our models and the
rest will be use for training.

We opted for this very simple method because more complicated k-cross
validation has a very long runtime (as the number of observations is
pretty high). Thus our prediction of the accuracy of the model may be
biaised.

Also for some models we will use the 300 first pca components instead of
the whole set for the training.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train.tr }\OtherTok{\textless{}{-}}\NormalTok{ train[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{300}\NormalTok{),]}
\NormalTok{train.te }\OtherTok{\textless{}{-}}\NormalTok{ train[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{300}\NormalTok{),]}
\NormalTok{train.pred }\OtherTok{\textless{}{-}}\NormalTok{ train[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{300}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{501}\NormalTok{)]}

\NormalTok{train.pca }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(pca}\SpecialCharTok{$}\NormalTok{x[,}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{300}\NormalTok{)], }\FunctionTok{as.logical}\NormalTok{(train.pred)))}
\NormalTok{train.pca.tr }\OtherTok{\textless{}{-}}\NormalTok{ train.pca[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{300}\NormalTok{),]}
\NormalTok{train.pca.te }\OtherTok{\textless{}{-}}\NormalTok{ train.pca[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{300}\NormalTok{),]}
\NormalTok{train.pca.pred }\OtherTok{\textless{}{-}}\NormalTok{ train[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{300}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{501}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

Logistic prediction:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_logistic\_pred }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(model, data, }\AttributeTok{pos =} \DecValTok{1}\NormalTok{, }\AttributeTok{neg =} \DecValTok{0}\NormalTok{, }\AttributeTok{cut =} \FloatTok{0.5}\NormalTok{) \{}
\NormalTok{  probs }\OtherTok{=} \FunctionTok{predict}\NormalTok{(model, }\AttributeTok{newdata =}\NormalTok{ data)}
  \FunctionTok{ifelse}\NormalTok{(probs }\SpecialCharTok{\textgreater{}}\NormalTok{ cut, pos, neg)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm\_model }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(target}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ train.tr, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\StringTok{"logit"}\NormalTok{))}
\NormalTok{glm\_prediction }\OtherTok{\textless{}{-}} \FunctionTok{get\_logistic\_pred}\NormalTok{(glm\_model, train.te)}
\FunctionTok{mean}\NormalTok{(glm\_prediction }\SpecialCharTok{==}\NormalTok{ train.pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5133333
\end{verbatim}

This model gives a very bad prediction which is close to a random
prediction.

Perceptron:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{perceptron }\OtherTok{\textless{}{-}}\FunctionTok{nnet}\NormalTok{(target}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ train.tr, }\AttributeTok{skip =}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\DecValTok{0}\NormalTok{,}\AttributeTok{maxit=}\DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # weights:  501
## initial  value 593.000000 
## final  value 593.000000 
## converged
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{perceptron\_prediction }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(perceptron, train.te)}

\FunctionTok{mean}\NormalTok{(perceptron\_prediction }\SpecialCharTok{==}\NormalTok{ train.pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4766667
\end{verbatim}

The perceptron also give a precision close to random precision.

LDA:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lda\_model }\OtherTok{\textless{}{-}} \FunctionTok{lda}\NormalTok{(target}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ train.tr)}
\NormalTok{lda\_prediction }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(lda\_model, train.te)}
\FunctionTok{mean}\NormalTok{(lda\_prediction}\SpecialCharTok{$}\NormalTok{class }\SpecialCharTok{==}\NormalTok{ train.pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.54
\end{verbatim}

LDA in the whole data gives a result which is bearly better than the
other ones.

LDA on the projected data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lda\_model\_2 }\OtherTok{\textless{}{-}} \FunctionTok{lda}\NormalTok{(train.pca.tr[,}\FunctionTok{c}\NormalTok{(}\DecValTok{301}\NormalTok{)]}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ train.pca.tr[,}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{301}\NormalTok{)])}
\NormalTok{lda\_prediction\_2 }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(lda\_model\_2, train.pca.te)}
\FunctionTok{mean}\NormalTok{(lda\_prediction\_2}\SpecialCharTok{$}\NormalTok{class }\SpecialCharTok{==}\NormalTok{ train.pca.pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5466667
\end{verbatim}

With pca before lda the result is slightly better but still not very
encouraging.

knn:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{normalize }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{ (x }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(x))}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{max}\NormalTok{(x) }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(x))\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train.tr.norm }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(train.tr, normalize)}
\NormalTok{knn\_prediction }\OtherTok{\textless{}{-}} \FunctionTok{knn}\NormalTok{(train.tr, train.te, train.tr[,}\DecValTok{501}\NormalTok{], }\AttributeTok{k=}\DecValTok{18}\NormalTok{)}
\FunctionTok{mean}\NormalTok{(knn\_prediction }\SpecialCharTok{==}\NormalTok{ train.pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7666667
\end{verbatim}

This prediction is the best we got so far.

knn with projected data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train.pca.tr.norm }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(train.pca.tr, normalize)}
\NormalTok{knn\_prediction }\OtherTok{\textless{}{-}} \FunctionTok{knn}\NormalTok{(train.pca.tr, train.pca.te,}\AttributeTok{cl =}\NormalTok{ train.pca.tr[,}\FunctionTok{c}\NormalTok{(}\DecValTok{301}\NormalTok{)], }\AttributeTok{k=}\DecValTok{300}\NormalTok{)}
\FunctionTok{mean}\NormalTok{(knn\_prediction }\SpecialCharTok{==}\NormalTok{ train.pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6
\end{verbatim}

Surprisingly this gives a result which is worst than for the unprojected
data. A possible reason for this is that the pca we performed does not
explain enough of the data to get a correct result with the knn method.

\end{document}
